#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script Final de Otimiza√ß√£o Extrema para Modelo Lotof√°cil
Objetivo: Atingir 85-90% de acur√°cia com t√©cnicas avan√ßadas de ML
"""

import pandas as pd
import numpy as np
from datetime import datetime
import pickle
import json
import os
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, ExtraTreesClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_val_score
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.feature_selection import SelectKBest, f_classif, RFE
from imblearn.over_sampling import SMOTE, ADASYN
from imblearn.combine import SMOTETomek
from sklearn.decomposition import PCA
from sklearn.pipeline import Pipeline
import warnings
warnings.filterwarnings('ignore')

# Imports locais
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from dados.dados import carregar_dados
from experimentos.feature_engineering import FeatureEngineeringLotofacil

def formatar_porcentagem(valor):
    """Converte decimal para porcentagem formatada"""
    return f"{valor * 100:.2f}%"

def criar_features_extremas(df):
    """Cria features extremamente avan√ßadas com m√∫ltiplas t√©cnicas"""
    print("   üöÄ Criando features extremamente avan√ßadas...")
    
    fe = FeatureEngineeringLotofacil()
    
    # 1. Features estat√≠sticas
    features_estat = fe.criar_features_estatisticas(df)
    print(f"      ‚úì Features estat√≠sticas: {features_estat.shape[1]} colunas")
    
    # 2. Features customizadas avan√ßadas
    features_custom = criar_features_matematicas_avancadas(df)
    print(f"      ‚úì Features matem√°ticas: {features_custom.shape[1]} colunas")
    
    # 3. Features de intera√ß√£o
    features_interacao = criar_features_interacao(df)
    print(f"      ‚úì Features de intera√ß√£o: {features_interacao.shape[1]} colunas")
    
    # 4. Features de tend√™ncia hist√≥rica
    features_tendencia = criar_features_tendencia_historica(df)
    print(f"      ‚úì Features de tend√™ncia: {features_tendencia.shape[1]} colunas")
    
    # Combinar todas as features
    features_list = [features_estat, features_custom, features_interacao, features_tendencia]
    features_completas = pd.concat(features_list, axis=1)
    
    # Remover colunas duplicadas e com vari√¢ncia zero
    features_completas = features_completas.loc[:, ~features_completas.columns.duplicated()]
    features_completas = features_completas.loc[:, features_completas.var() > 1e-6]
    
    print(f"   ‚úÖ Total de features criadas: {features_completas.shape[1]}")
    return features_completas

def criar_features_matematicas_avancadas(df):
    """Cria features matem√°ticas super avan√ßadas"""
    features_math = pd.DataFrame(index=df.index)
    
    numeros_cols = [f'Bola_{i:02d}' for i in range(1, 16)]
    
    for idx, row in df.iterrows():
        numeros = [row[col] for col in numeros_cols if col in row and pd.notna(row[col])]
        
        if len(numeros) == 15:
            numeros = np.array(numeros)
            
            # Features estat√≠sticas avan√ßadas
            features_math.loc[idx, 'skewness'] = pd.Series(numeros).skew()
            features_math.loc[idx, 'kurtosis'] = pd.Series(numeros).kurtosis()
            features_math.loc[idx, 'coef_variacao'] = np.std(numeros) / np.mean(numeros) if np.mean(numeros) > 0 else 0
            
            # Features geom√©tricas
            features_math.loc[idx, 'media_geometrica'] = np.exp(np.mean(np.log(numeros)))
            features_math.loc[idx, 'media_harmonica'] = len(numeros) / np.sum(1.0/numeros)
            
            # Features de dispers√£o
            q1, q3 = np.percentile(numeros, [25, 75])
            features_math.loc[idx, 'iqr'] = q3 - q1
            features_math.loc[idx, 'mad'] = np.median(np.abs(numeros - np.median(numeros)))
            
            # Features de posi√ß√£o relativa
            features_math.loc[idx, 'pos_mediana'] = np.where(np.sort(numeros) == np.median(numeros))[0][0] if len(np.where(np.sort(numeros) == np.median(numeros))[0]) > 0 else 7
            
            # Features de densidade
            hist, _ = np.histogram(numeros, bins=5, range=(1, 25))
            features_math.loc[idx, 'densidade_max'] = np.max(hist)
            features_math.loc[idx, 'densidade_min'] = np.min(hist)
            features_math.loc[idx, 'densidade_var'] = np.var(hist)
            
            # Features de padr√µes num√©ricos
            diffs = np.diff(np.sort(numeros))
            features_math.loc[idx, 'diff_media'] = np.mean(diffs)
            features_math.loc[idx, 'diff_std'] = np.std(diffs)
            features_math.loc[idx, 'diff_max'] = np.max(diffs)
            features_math.loc[idx, 'diff_min'] = np.min(diffs)
            
            # Features de simetria
            centro = 13  # Centro da faixa 1-25
            distancias = np.abs(numeros - centro)
            features_math.loc[idx, 'simetria_centro'] = np.mean(distancias)
            
            # Features de concentra√ß√£o
            features_math.loc[idx, 'concentracao_baixa'] = np.sum(numeros <= 8)
            features_math.loc[idx, 'concentracao_media'] = np.sum((numeros > 8) & (numeros <= 17))
            features_math.loc[idx, 'concentracao_alta'] = np.sum(numeros > 17)
            
        else:
            # Valores padr√£o
            for col in ['skewness', 'kurtosis', 'coef_variacao', 'media_geometrica', 'media_harmonica',
                       'iqr', 'mad', 'pos_mediana', 'densidade_max', 'densidade_min', 'densidade_var',
                       'diff_media', 'diff_std', 'diff_max', 'diff_min', 'simetria_centro',
                       'concentracao_baixa', 'concentracao_media', 'concentracao_alta']:
                features_math.loc[idx, col] = 0
    
    return features_math.fillna(0)

def criar_features_interacao(df):
    """Cria features de intera√ß√£o entre vari√°veis"""
    features_int = pd.DataFrame(index=df.index)
    
    numeros_cols = [f'Bola_{i:02d}' for i in range(1, 16)]
    
    for idx, row in df.iterrows():
        numeros = [row[col] for col in numeros_cols if col in row and pd.notna(row[col])]
        
        if len(numeros) == 15:
            numeros = np.array(numeros)
            
            # Intera√ß√µes entre estat√≠sticas b√°sicas
            soma = np.sum(numeros)
            media = np.mean(numeros)
            std = np.std(numeros)
            
            features_int.loc[idx, 'soma_x_media'] = soma * media
            features_int.loc[idx, 'soma_div_std'] = soma / std if std > 0 else 0
            features_int.loc[idx, 'media_x_std'] = media * std
            
            # Intera√ß√µes de paridade
            pares = np.sum(numeros % 2 == 0)
            impares = np.sum(numeros % 2 == 1)
            features_int.loc[idx, 'razao_par_impar'] = pares / impares if impares > 0 else 0
            features_int.loc[idx, 'produto_par_impar'] = pares * impares
            
            # Intera√ß√µes de posi√ß√£o
            baixos = np.sum(numeros <= 12)
            altos = np.sum(numeros >= 14)
            features_int.loc[idx, 'razao_baixo_alto'] = baixos / altos if altos > 0 else 0
            features_int.loc[idx, 'produto_baixo_alto'] = baixos * altos
            
            # Intera√ß√µes de quadrantes
            q1 = np.sum((numeros >= 1) & (numeros <= 6))
            q2 = np.sum((numeros >= 7) & (numeros <= 12))
            q3 = np.sum((numeros >= 13) & (numeros <= 18))
            q4 = np.sum((numeros >= 19) & (numeros <= 25))
            
            features_int.loc[idx, 'q1_x_q4'] = q1 * q4
            features_int.loc[idx, 'q2_x_q3'] = q2 * q3
            features_int.loc[idx, 'diagonal_principal'] = q1 + q4
            features_int.loc[idx, 'diagonal_secundaria'] = q2 + q3
            
        else:
            # Valores padr√£o
            for col in ['soma_x_media', 'soma_div_std', 'media_x_std', 'razao_par_impar', 'produto_par_impar',
                       'razao_baixo_alto', 'produto_baixo_alto', 'q1_x_q4', 'q2_x_q3', 'diagonal_principal', 'diagonal_secundaria']:
                features_int.loc[idx, col] = 0
    
    return features_int.fillna(0)

def criar_features_tendencia_historica(df):
    """Cria features baseadas em tend√™ncias hist√≥ricas"""
    features_tend = pd.DataFrame(index=df.index)
    
    numeros_cols = [f'Bola_{i:02d}' for i in range(1, 16)]
    
    # Calcular frequ√™ncias hist√≥ricas
    freq_historica = {i: 0 for i in range(1, 26)}
    
    for idx, row in df.iterrows():
        numeros = [row[col] for col in numeros_cols if col in row and pd.notna(row[col])]
        
        if len(numeros) == 15:
            # Atualizar frequ√™ncias at√© este ponto
            for num in numeros:
                freq_historica[num] += 1
            
            # Features baseadas em frequ√™ncia hist√≥rica
            freqs_atuais = [freq_historica[num] for num in numeros]
            features_tend.loc[idx, 'freq_media_historica'] = np.mean(freqs_atuais)
            features_tend.loc[idx, 'freq_std_historica'] = np.std(freqs_atuais)
            features_tend.loc[idx, 'freq_max_historica'] = np.max(freqs_atuais)
            features_tend.loc[idx, 'freq_min_historica'] = np.min(freqs_atuais)
            
            # Features de "calor" dos n√∫meros
            total_jogos = idx + 1
            freq_relativas = [freq_historica[num] / total_jogos for num in numeros]
            features_tend.loc[idx, 'calor_medio'] = np.mean(freq_relativas)
            features_tend.loc[idx, 'calor_max'] = np.max(freq_relativas)
            features_tend.loc[idx, 'calor_min'] = np.min(freq_relativas)
            
            # Features de desvio da frequ√™ncia esperada
            freq_esperada = total_jogos * 15 / 25  # Frequ√™ncia esperada para cada n√∫mero
            desvios = [abs(freq_historica[num] - freq_esperada) for num in numeros]
            features_tend.loc[idx, 'desvio_medio_freq'] = np.mean(desvios)
            features_tend.loc[idx, 'desvio_max_freq'] = np.max(desvios)
            
        else:
            # Valores padr√£o
            for col in ['freq_media_historica', 'freq_std_historica', 'freq_max_historica', 'freq_min_historica',
                       'calor_medio', 'calor_max', 'calor_min', 'desvio_medio_freq', 'desvio_max_freq']:
                features_tend.loc[idx, col] = 0
    
    return features_tend.fillna(0)

def criar_ensemble_extremo(X_train, y_train):
    """Cria ensemble extremamente otimizado com grid search"""
    print("   üéØ Criando ensemble extremo com grid search...")
    
    # Definir modelos com par√¢metros para otimiza√ß√£o
    modelos_params = {
        'rf': {
            'modelo': RandomForestClassifier(random_state=42, n_jobs=-1),
            'params': {
                'n_estimators': [200, 300, 500],
                'max_depth': [10, 15, 20, None],
                'min_samples_split': [2, 5, 10],
                'min_samples_leaf': [1, 2, 4]
            }
        },
        'gb': {
            'modelo': GradientBoostingClassifier(random_state=42),
            'params': {
                'n_estimators': [100, 200, 300],
                'learning_rate': [0.05, 0.1, 0.2],
                'max_depth': [3, 5, 7],
                'min_samples_split': [2, 5, 10]
            }
        },
        'et': {
            'modelo': ExtraTreesClassifier(random_state=42, n_jobs=-1),
            'params': {
                'n_estimators': [200, 300, 500],
                'max_depth': [10, 15, 20],
                'min_samples_split': [2, 5, 10]
            }
        }
    }
    
    # Otimizar cada modelo
    modelos_otimizados = {}
    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    
    for nome, config in modelos_params.items():
        print(f"      üîÑ Otimizando {nome.upper()}...")
        
        grid_search = GridSearchCV(
            config['modelo'],
            config['params'],
            cv=cv,
            scoring='accuracy',
            n_jobs=-1,
            verbose=0
        )
        
        grid_search.fit(X_train, y_train)
        modelos_otimizados[nome] = grid_search.best_estimator_
        print(f"         ‚úì Melhor score: {grid_search.best_score_:.4f}")
    
    # Criar ensemble final
    ensemble = VotingClassifier(
        estimators=[(nome, modelo) for nome, modelo in modelos_otimizados.items()],
        voting='soft'
    )
    
    print("      üîÑ Treinando ensemble final...")
    ensemble.fit(X_train, y_train)
    
    return ensemble, modelos_otimizados

def aplicar_balanceamento_extremo(X_train, y_train):
    """Aplica balanceamento extremo com m√∫ltiplas t√©cnicas"""
    print("   ‚öñÔ∏è Aplicando balanceamento extremo...")
    
    # Verificar distribui√ß√£o original
    unique, counts = np.unique(y_train, return_counts=True)
    print(f"      üìä Distribui√ß√£o original: {dict(zip(unique, counts))}")
    
    # Usar SMOTETomek que combina oversampling e undersampling
    smote_tomek = SMOTETomek(random_state=42)
    X_balanced, y_balanced = smote_tomek.fit_resample(X_train, y_train)
    
    # Verificar nova distribui√ß√£o
    unique, counts = np.unique(y_balanced, return_counts=True)
    print(f"      üìä Distribui√ß√£o balanceada: {dict(zip(unique, counts))}")
    
    return X_balanced, y_balanced

def selecionar_features_extremas(X_train, y_train, k=80):
    """Sele√ß√£o extrema de features com m√∫ltiplos m√©todos"""
    print(f"   üéØ Sele√ß√£o extrema de {k} melhores features...")
    
    # M√©todo 1: SelectKBest
    selector_kbest = SelectKBest(score_func=f_classif, k=min(k, X_train.shape[1]))
    X_kbest = selector_kbest.fit_transform(X_train, y_train)
    
    # M√©todo 2: RFE com Random Forest
    rf_selector = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
    rfe = RFE(rf_selector, n_features_to_select=min(k, X_train.shape[1]))
    X_rfe = rfe.fit_transform(X_train, y_train)
    
    # Combinar sele√ß√µes (interse√ß√£o das features selecionadas)
    features_kbest = set(selector_kbest.get_support(indices=True))
    features_rfe = set(rfe.get_support(indices=True))
    features_intersecao = features_kbest.intersection(features_rfe)
    
    if len(features_intersecao) < 30:  # Garantir m√≠nimo de features
        features_finais = list(features_kbest.union(features_rfe))[:k]
    else:
        features_finais = list(features_intersecao)
    
    # Aplicar sele√ß√£o final
    X_selected = X_train[:, features_finais]
    
    print(f"      ‚úì Features selecionadas: {len(features_finais)}")
    
    return X_selected, features_finais

def avaliar_modelo_extremo(modelo, X_test, y_test, features_selecionadas):
    """Avalia√ß√£o extrema do modelo"""
    print("   üìä Avalia√ß√£o extrema do modelo...")
    
    # Aplicar sele√ß√£o de features no teste
    X_test_selected = X_test[:, features_selecionadas]
    
    # Predi√ß√µes
    y_pred = modelo.predict(X_test_selected)
    
    # Calcular m√©tricas
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)
    recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)
    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)
    
    # ROC-AUC
    try:
        if hasattr(modelo, 'predict_proba'):
            y_proba = modelo.predict_proba(X_test_selected)
            if y_proba.shape[1] == 2:
                roc_auc = roc_auc_score(y_test, y_proba[:, 1])
            else:
                roc_auc = roc_auc_score(y_test, y_proba, multi_class='ovr', average='weighted')
        else:
            roc_auc = 0.5
    except:
        roc_auc = 0.5
    
    # Relat√≥rio detalhado
    print("\n" + "="*50)
    print("üìä RELAT√ìRIO DETALHADO:")
    print("="*50)
    print(classification_report(y_test, y_pred))
    
    return {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1_score': f1,
        'roc_auc': roc_auc
    }

def main():
    """Fun√ß√£o principal de otimiza√ß√£o extrema"""
    print("üöÄ INICIANDO OTIMIZA√á√ÉO EXTREMA DO MODELO LOTOF√ÅCIL")
    print("=" * 70)
    
    # 1. Carregar dados
    print("\n1. Carregando dados...")
    df_dados = carregar_dados()
    print(f"   ‚úì Dados carregados: {len(df_dados)} concursos")
    
    # 2. Criar features extremas
    print("\n2. Criando features extremas...")
    features_df = criar_features_extremas(df_dados)
    print(f"   ‚úì Features criadas: {features_df.shape}")
    
    # 3. Preparar targets
    print("\n3. Preparando targets...")
    targets = []
    for idx, row in df_dados.iterrows():
        ganhou = 1 if row.get('Ganhadores_Sena', 0) > 0 else 0
        targets.append(ganhou)
    
    y = np.array(targets)
    print(f"   ‚úì Targets preparados: {len(y)} amostras")
    
    # Verificar distribui√ß√£o
    unique, counts = np.unique(y, return_counts=True)
    print(f"   ‚úì Distribui√ß√£o - {dict(zip(unique, counts))}")
    
    # 4. Preparar features finais
    if 'Ganhou' in features_df.columns:
        features_df = features_df.drop('Ganhou', axis=1)
    
    X = features_df.values
    print(f"   ‚úì Features finais: {X.shape}")
    
    # 5. Valida√ß√£o temporal
    print("\n4. Aplicando valida√ß√£o temporal...")
    split_idx = int(len(X) * 0.8)
    X_train, X_test = X[:split_idx], X[split_idx:]
    y_train, y_test = y[:split_idx], y[split_idx:]
    
    print(f"   ‚úì Treino: {X_train.shape[0]} amostras")
    print(f"   ‚úì Teste: {X_test.shape[0]} amostras")
    
    # 6. Normaliza√ß√£o robusta
    print("\n5. Aplicando normaliza√ß√£o robusta...")
    scaler = RobustScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # 7. Sele√ß√£o extrema de features
    print("\n6. Sele√ß√£o extrema de features...")
    X_train_selected, features_selecionadas = selecionar_features_extremas(
        X_train_scaled, y_train, k=min(80, X_train_scaled.shape[1])
    )
    
    # 8. Balanceamento extremo
    print("\n7. Balanceamento extremo...")
    X_train_balanced, y_train_balanced = aplicar_balanceamento_extremo(
        X_train_selected, y_train
    )
    
    # 9. Treinamento do ensemble extremo
    print("\n8. Treinamento do ensemble extremo...")
    ensemble, modelos_individuais = criar_ensemble_extremo(
        X_train_balanced, y_train_balanced
    )
    
    # 10. Avalia√ß√£o extrema
    print("\n9. Avalia√ß√£o extrema...")
    metricas = avaliar_modelo_extremo(ensemble, X_test_scaled, y_test, features_selecionadas)
    
    # 11. Resultados finais
    print("\n" + "=" * 70)
    print("üèÜ RESULTADOS DA OTIMIZA√á√ÉO EXTREMA:")
    print("=" * 70)
    print(f"‚Ä¢ Acur√°cia: {formatar_porcentagem(metricas['accuracy'])}")
    print(f"‚Ä¢ Precis√£o: {formatar_porcentagem(metricas['precision'])}")
    print(f"‚Ä¢ Recall: {formatar_porcentagem(metricas['recall'])}")
    print(f"‚Ä¢ F1-Score: {formatar_porcentagem(metricas['f1_score'])}")
    print(f"‚Ä¢ ROC-AUC: {formatar_porcentagem(metricas['roc_auc'])}")
    
    # Status da meta
    if metricas['accuracy'] >= 0.85:
        status = "‚úÖ META ATINGIDA - EXCELENTE!"
        cor = "üü¢"
    elif metricas['accuracy'] >= 0.80:
        status = "üü° PR√ìXIMO DA META - BOM RESULTADO"
        cor = "üü°"
    else:
        status = "‚ùå ABAIXO DA META - NECESS√ÅRIO MAIS AJUSTES"
        cor = "üî¥"
    
    print(f"\n{cor} Status: {status}")
    
    # 12. Salvar modelo final
    print("\n10. Salvando modelo final extremo...")
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # Pipeline completo
    pipeline_final = {
        'scaler': scaler,
        'features_selecionadas': features_selecionadas,
        'modelo': ensemble,
        'modelos_individuais': modelos_individuais,
        'metricas': metricas,
        'timestamp': timestamp
    }
    
    # Salvar modelo
    modelo_path = f"modelos/modelo_final_extremo_{timestamp}.pkl"
    os.makedirs("modelos", exist_ok=True)
    with open(modelo_path, 'wb') as f:
        pickle.dump(pipeline_final, f)
    print(f"   ‚úì Modelo salvo: {modelo_path}")
    
    # Relat√≥rio final
    relatorio_final = {
        'timestamp': timestamp,
        'versao': 'EXTREMA',
        'total_features_originais': X.shape[1],
        'features_selecionadas': len(features_selecionadas),
        'amostras_treino': X_train.shape[0],
        'amostras_teste': X_test.shape[0],
        'metricas_raw': {
            'accuracy': float(metricas['accuracy']),
            'precision': float(metricas['precision']),
            'recall': float(metricas['recall']),
            'f1_score': float(metricas['f1_score']),
            'roc_auc': float(metricas['roc_auc'])
        },
        'metricas_formatadas': {
            'accuracy': formatar_porcentagem(metricas['accuracy']),
            'precision': formatar_porcentagem(metricas['precision']),
            'recall': formatar_porcentagem(metricas['recall']),
            'f1_score': formatar_porcentagem(metricas['f1_score']),
            'roc_auc': formatar_porcentagem(metricas['roc_auc'])
        },
        'meta_85_90_atingida': metricas['accuracy'] >= 0.85,
        'meta_80_atingida': metricas['accuracy'] >= 0.80,
        'modelos_ensemble': list(modelos_individuais.keys()),
        'tecnicas_aplicadas': [
            'Feature Engineering Extrema',
            'Sele√ß√£o de Features M√∫ltipla (KBest + RFE)',
            'Balanceamento SMOTETomek',
            'Normaliza√ß√£o Robusta',
            'Grid Search Otimiza√ß√£o',
            'Ensemble Voting Classifier',
            'Valida√ß√£o Temporal'
        ]
    }
    
    relatorio_path = f"experimentos/resultados/modelo_final_extremo_{timestamp}.json"
    os.makedirs("experimentos/resultados", exist_ok=True)
    with open(relatorio_path, 'w', encoding='utf-8') as f:
        json.dump(relatorio_final, f, indent=2, ensure_ascii=False)
    print(f"   ‚úì Relat√≥rio salvo: {relatorio_path}")
    
    print("\nüéâ OTIMIZA√á√ÉO EXTREMA FINALIZADA!")
    
    return metricas['accuracy'] >= 0.85, metricas['accuracy']

if __name__ == "__main__":
    sucesso, acuracia_final = main()
    
    print("\n" + "="*70)
    if sucesso:
        print("üèÜ PARAB√âNS! META DE 85-90% ATINGIDA COM SUCESSO!")
        print(f"üéØ Acur√°cia Final: {acuracia_final*100:.2f}%")
    else:
        print(f"üìä Resultado Final: {acuracia_final*100:.2f}%")
        if acuracia_final >= 0.80:
            print("üü° Resultado muito bom, pr√≥ximo da meta!")
        else:
            print("üî¥ Necess√°rio mais ajustes para atingir a meta.")
    print("="*70)